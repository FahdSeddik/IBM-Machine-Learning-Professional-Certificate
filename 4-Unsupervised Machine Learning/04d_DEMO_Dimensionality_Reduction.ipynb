{"cells":[{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# Machine Learning Foundation\n","\n","## Course 4, Part d: Dimensionality Reduction DEMO\n"]},{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","We will be using customer data from a [Portuguese wholesale distributor](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) for clustering. This data file is called `Wholesale_Customers_Data`.\n","\n","It contains the following features:\n","\n","*   Fresh: annual spending (m.u.) on fresh products\n","*   Milk: annual spending (m.u.) on milk products\n","*   Grocery: annual spending (m.u.) on grocery products\n","*   Frozen: annual spending (m.u.) on frozen products\n","*   Detergents_Paper: annual spending (m.u.) on detergents and paper products\n","*   Delicatessen: annual spending (m.u.) on delicatessen products\n","*   Channel: customer channel (1: hotel/restaurant/cafe or 2: retail)\n","*   Region: customer region (1: Lisbon, 2: Porto, 3: Other)\n","\n","In this data, the values for all spending are given in an arbitrary unit (m.u. = monetary unit).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","import seaborn as sns, pandas as pd, numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os, pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n"]},{"cell_type":"markdown","metadata":{},"source":["## Part 1\n","\n","In this section, we will:\n","\n","*   Import the data and check the data types.\n","*   Drop the channel and region columns as they won't be used since we focus on numeric columns for this example.\n","*   Convert the remaining columns to floats if necessary.\n","*   Copy this version of the data (using the `copy` method) to a variable to preserve it. We will be using it later.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%203/data/Wholesale_Customers_Data.csv', sep=',')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = data.drop(['Channel', 'Region'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["data.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert to floats\n","for col in data.columns:\n","    data[col] = data[col].astype(np.float)"]},{"cell_type":"markdown","metadata":{},"source":["Preserve the original data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_orig = data.copy()"]},{"cell_type":"markdown","metadata":{},"source":["## Part 2\n","\n","As with the previous lesson, we need to ensure the data is scaled and (relatively) normally distributed.\n","\n","*   Examine the correlation and skew.\n","*   Perform any transformations and scale data using your favorite scaling method.\n","*   View the pairwise correlation plots of the new data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["corr_mat = data.corr()\n","\n","# Strip the diagonal for future examination\n","for x in range(corr_mat.shape[0]):\n","    corr_mat.iloc[x,x] = 0.0\n","    \n","corr_mat"]},{"cell_type":"markdown","metadata":{},"source":["As before, the two categories with their respective most strongly correlated variable.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["corr_mat.abs().idxmax()"]},{"cell_type":"markdown","metadata":{},"source":["Examine the skew values and log transform. Looks like all of them need it.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["log_columns = data.skew().sort_values(ascending=False)\n","log_columns = log_columns.loc[log_columns > 0.75]\n","\n","log_columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The log transformations\n","for col in log_columns.index:\n","    data[col] = np.log1p(data[col])"]},{"cell_type":"markdown","metadata":{},"source":["Scale the data again. Let's use `MinMaxScaler` this time just to mix things up.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","mms = MinMaxScaler()\n","\n","for col in data.columns:\n","    data[col] = mms.fit_transform(data[[col]]).squeeze()"]},{"cell_type":"markdown","metadata":{},"source":["Visualize the relationship between the variables.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["sns.set_context('notebook')\n","sns.set_style('white')\n","sns.pairplot(data);"]},{"cell_type":"markdown","metadata":{},"source":["## Part 3\n","\n","In this section, we will:\n","\n","*   Using Scikit-learn's [pipeline function](http://scikit-learn.org/stable/modules/pipeline.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01), recreate the data pre-processing scheme above (transformation and scaling) using a pipeline. If you used a non-Scikit learn function to transform the data (e.g. NumPy's log function), checkout  the custom transformer class called [`FunctionTransformer`](http://scikit-learn.org/stable/modules/preprocessing.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01#custom-transformers).\n","*   Use the pipeline to transform the original data that was stored at the end of question 1.\n","*   Compare the results to the original data to verify that everything worked.\n","\n","*Note:* Scikit-learn has a more flexible `Pipeline` function and a shortcut version called `make_pipeline`. Either can be used. Also, if different transformations need to be performed on the data, a [`FeatureUnion`](http://scikit-learn.org/stable/modules/pipeline.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01#featureunion-composite-feature-spaces) can be used.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from sklearn.preprocessing import FunctionTransformer\n","from sklearn.pipeline import Pipeline\n","\n","# The custom NumPy log transformer\n","log_transformer = FunctionTransformer(np.log1p)\n","\n","# The pipeline\n","estimators = [('log1p', log_transformer), ('minmaxscale', MinMaxScaler())]\n","pipeline = Pipeline(estimators)\n","\n","# Convert the original data\n","data_pipe = pipeline.fit_transform(data_orig)"]},{"cell_type":"markdown","metadata":{},"source":["The results are identical. Note that machine learning models and grid searches can also be added to the pipeline (and in fact, usually are.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["np.allclose(data_pipe, data)"]},{"cell_type":"markdown","metadata":{},"source":["## Part 4\n","\n","In this section, we will:\n","\n","*   Perform PCA with `n_components` ranging from 1 to 5.\n","*   Store the amount of explained variance for each number of dimensions.\n","*   Also store the feature importance for each number of dimensions. *Hint:* PCA doesn't explicitly provide this after a model is fit, but the `components_` properties can be used to determine something that approximates importance. How you decided to do so is entirely up to you.\n","*   Plot the explained variance and feature importances.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","pca_list = list()\n","feature_weight_list = list()\n","\n","# Fit a range of PCA models\n","\n","for n in range(1, 6):\n","    \n","    # Create and fit the model\n","    PCAmod = PCA(n_components=n)\n","    PCAmod.fit(data)\n","    \n","    # Store the model and variance\n","    pca_list.append(pd.Series({'n':n, 'model':PCAmod,\n","                               'var': PCAmod.explained_variance_ratio_.sum()}))\n","    \n","    # Calculate and store feature importances\n","    abs_feature_values = np.abs(PCAmod.components_).sum(axis=0)\n","    feature_weight_list.append(pd.DataFrame({'n':n, \n","                                             'features': data.columns,\n","                                             'values':abs_feature_values/abs_feature_values.sum()}))\n","    \n","pca_df = pd.concat(pca_list, axis=1).T.set_index('n')\n","pca_df"]},{"cell_type":"markdown","metadata":{},"source":["Create a table of feature importances for each data column.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["features_df = (pd.concat(feature_weight_list)\n","               .pivot(index='n', columns='features', values='values'))\n","\n","features_df"]},{"cell_type":"markdown","metadata":{},"source":["Create a plot of explained variances.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["sns.set_context('talk')\n","ax = pca_df['var'].plot(kind='bar')\n","\n","ax.set(xlabel='Number of dimensions',\n","       ylabel='Percent explained variance',\n","       title='Explained Variance vs Dimensions');"]},{"cell_type":"markdown","metadata":{},"source":["And here's a plot of feature importances.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["ax = features_df.plot(kind='bar', figsize=(13,8))\n","ax.legend(loc='upper right')\n","ax.set(xlabel='Number of dimensions',\n","       ylabel='Relative importance',\n","       title='Feature importance vs Dimensions');"]},{"cell_type":"markdown","metadata":{},"source":["This concludes \"Demo lab: Dimensionality Reduction (Part 1)\". We will be going over the remaining parts in the next module.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Part 5\n","\n","In this section, we will:\n","\n","*   Fit a `KernelPCA` model with `kernel='rbf'`. You can choose how many components and what values to use for the other parameters (`rbf` refers to a Radial Basis Function kernel, and the `gamma` parameter governs scaling of this kernel and typically ranges between 0 and 1). Several other [kernels](https://scikit-learn.org/stable/modules/metrics.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01) can be tried, and even passed ss cross validation parameters (see this [example](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01)).\n","*   If you want to tinker some more, use `GridSearchCV` to tune the parameters of the `KernelPCA` model.\n","\n","The second step is tricky since grid searches are generally used for supervised machine learning methods and rely on scoring metrics, such as accuracy, to determine the best model. However, a custom scoring function can be written for `GridSearchCV`, where larger is better for the outcome of the scoring function.\n","\n","What would such a metric involve for PCA? What about percent of explained variance? Or perhaps the negative mean squared error on the data once it has been transformed and then inversely transformed?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from sklearn.decomposition import KernelPCA\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error\n","\n","# Custom scorer--use negative rmse of inverse transform\n","def scorer(pcamodel, X, y=None):\n","\n","    try:\n","        X_val = X.values\n","    except:\n","        X_val = X\n","        \n","    # Calculate and inverse transform the data\n","    data_inv = pcamodel.fit(X_val).transform(X_val)\n","    data_inv = pcamodel.inverse_transform(data_inv)\n","    \n","    # The error calculation\n","    mse = mean_squared_error(data_inv.ravel(), X_val.ravel())\n","    \n","    # Larger values are better for scorers, so take negative value\n","    return -1.0 * mse\n","\n","# The grid search parameters\n","param_grid = {'gamma':[0.001, 0.01, 0.05, 0.1, 0.5, 1.0],\n","              'n_components': [2, 3, 4]}\n","\n","# The grid search\n","kernelPCA = GridSearchCV(KernelPCA(kernel='rbf', fit_inverse_transform=True),\n","                         param_grid=param_grid,\n","                         scoring=scorer,\n","                         n_jobs=-1)\n","\n","\n","kernelPCA = kernelPCA.fit(data)\n","\n","kernelPCA.best_estimator_"]},{"cell_type":"markdown","metadata":{},"source":["## Part 6\n","\n","Let's explore how our model accuracy may change if we include a `PCA` in our model building pipeline. Let's plan to use sklearn's `Pipeline` class and create a pipeline that has the following steps:\n","\n","<ol>\n","  <li>A scaler</li>\n","  <li>`PCA(n_components=n)`</li>\n","  <li>`LogisticRegression`</li>\n","</ol>\n","\n","*   Load the Human Activity data from the datasets.\n","*   Write a function that takes in a value of `n` and makes the above pipeline, then predicts the \"Activity\" column over a 5-fold StratifiedShuffleSplit, and returns the average test accuracy\n","*   For various values of n, call the above function and store the average accuracies.\n","*   Plot the average accuracy by number of dimensions.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/Human_Activity_Recognition_Using_Smartphones_Data.csv', sep=',')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"scrolled":true},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","\n","X = data.drop('Activity', axis=1)\n","y = data.Activity\n","sss = StratifiedShuffleSplit(n_splits=5, random_state=42)\n","\n","def get_avg_score(n):\n","    pipe = [\n","        ('scaler', StandardScaler()),\n","        ('pca', PCA(n_components=n)),\n","        ('estimator', LogisticRegression(solver='liblinear'))\n","    ]\n","    pipe = Pipeline(pipe)\n","    scores = []\n","    for train_index, test_index in sss.split(X, y):\n","        X_train, X_test = X.loc[train_index], X.loc[test_index]\n","        y_train, y_test = y.loc[train_index], y.loc[test_index]\n","        pipe.fit(X_train, y_train)\n","        scores.append(accuracy_score(y_test, pipe.predict(X_test)))\n","    return np.mean(scores)\n","\n","\n","ns = [10, 20, 50, 100, 150, 200, 300, 400]\n","score_list = [get_avg_score(n) for n in ns]"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["sns.set_context('talk')\n","\n","ax = plt.axes()\n","ax.plot(ns, score_list)\n","ax.set(xlabel='Number of Dimensions',\n","       ylabel='Average Accuracy',\n","       title='LogisticRegression Accuracy vs Number of dimensions on the Human Activity Dataset')\n","ax.grid(True)"]},{"cell_type":"markdown","metadata":{},"source":["***\n","\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":4}