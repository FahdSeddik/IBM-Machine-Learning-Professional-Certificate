{"cells":[{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# Machine Learning Foundation\n","\n","## Course 4, Part c: Clustering Methods LAB\n"]},{"cell_type":"markdown","metadata":{},"source":["# Clustering Methods Exercises\n"]},{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","This lab uses a dataset on wine quality. The data set contains various chemical properties of wine, such as acidity, sugar, pH, and alcohol. It also contains a quality metric (3-9, with highest being better) and a color (red or white). The name of the file is `Wine_Quality_Data.csv`.\n","\n","We will be using the chemical properties (i.e. everything but quality and color) to cluster the wine. Though this is unsupervised learning, there are interesting semi-supervised extensions relating clustering results onto color and quality.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","import seaborn as sns, pandas as pd, numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n"]},{"cell_type":"markdown","metadata":{},"source":["## Question 1\n","\n","*   Import the data and examine the features.\n","*   Note which are continuous, categorical, and boolean.\n","*   How many entries are there for the two colors and range of qualities?\n","*   Make a histogram plot of the quality for each of the wine colors.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T12:10:48.585240Z","start_time":"2017-03-20T08:10:48.548076-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["### BEGIN SOLUTION\n","# Import the data\n","data = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%202/Wine_Quality_Data.csv\")\n","\n","data.head(4).T"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T12:09:58.538878Z","start_time":"2017-03-20T08:09:58.531714-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["data.shape"]},{"cell_type":"markdown","metadata":{},"source":["The data types for each entry. The implementation of K-means in Scikit-learn is designed only to work with continuous data (even though it is sometimes used with categorical or boolean types). Fortunately, all the columns we will be using (everything except quality and color) are continuous.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T12:12:23.585637Z","start_time":"2017-03-20T08:12:23.576416-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["data.dtypes"]},{"cell_type":"markdown","metadata":{},"source":["The number of entries for each wine color.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T12:10:32.347631Z","start_time":"2017-03-20T08:10:32.337545-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["data.color.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["The distribution of quality values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T13:00:49.509254Z","start_time":"2017-03-20T09:00:49.495394-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["data.quality.value_counts().sort_index()"]},{"cell_type":"markdown","metadata":{},"source":["Now for the histogram.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T13:05:20.809064Z","start_time":"2017-03-20T09:05:20.584910-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["# seaborn styles\n","sns.set_context('notebook')\n","sns.set_style('white')\n","\n","# custom colors\n","red = sns.color_palette()[2]\n","white = sns.color_palette()[4]\n","\n","# set bins for histogram\n","bin_range = np.array([3, 4, 5, 6, 7, 8, 9])\n","\n","# plot histogram of quality counts for red and white wines\n","ax = plt.axes()\n","for color, plot_color in zip(['red', 'white'], [red, white]):\n","    q_data = data.loc[data.color==color, 'quality']\n","    q_data.hist(bins=bin_range, \n","                alpha=0.5, ax=ax, \n","                color=plot_color, label=color)\n","    \n","\n","ax.legend()\n","ax.set(xlabel='Quality', ylabel='Occurrence')\n","\n","# force tick labels to be in middle of region\n","ax.set_xlim(3,10)\n","ax.set_xticks(bin_range+0.5)\n","ax.set_xticklabels(bin_range);\n","ax.grid('off')\n","### END SOLUTION"]},{"cell_type":"markdown","metadata":{},"source":["## Question 2\n","\n","*   Examine the correlation and skew of the relevant variables--everything except color and quality (without dropping these columns from our data).\n","*   Perform any appropriate feature transformations and/or scaling.\n","*   Examine the pairwise distribution of the variables with pairplots to verify scaling and normalization efforts.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["### BEGIN SOLUTION\n","float_columns = [x for x in data.columns if x not in ['color', 'quality']]\n","\n","# The correlation matrix\n","corr_mat = data[float_columns].corr()\n","\n","# Strip out the diagonal values for the next step\n","for x in range(len(float_columns)):\n","    corr_mat.iloc[x,x] = 0.0\n","    \n","corr_mat"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["# Pairwise maximal correlations\n","corr_mat.abs().idxmax()"]},{"cell_type":"markdown","metadata":{},"source":["And an examination of the skew values in anticipation of transformations.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["skew_columns = (data[float_columns]\n","                .skew()\n","                .sort_values(ascending=False))\n","\n","skew_columns = skew_columns.loc[skew_columns > 0.75]\n","skew_columns"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["# Perform log transform on skewed columns\n","for col in skew_columns.index.tolist():\n","    data[col] = np.log1p(data[col])\n"]},{"cell_type":"markdown","metadata":{},"source":["Perform feature scaling.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","sc = StandardScaler()\n","data[float_columns] = sc.fit_transform(data[float_columns])\n","\n","data.head(4)"]},{"cell_type":"markdown","metadata":{},"source":["Finally, the pairplot of the transformed and scaled features.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["sns.set_context('notebook')\n","sns.pairplot(data[float_columns + ['color']], \n","             hue='color', \n","             hue_order=['white', 'red'],\n","             palette={'red':red, 'white':'gray'});\n","### END SOLUTION"]},{"cell_type":"markdown","metadata":{},"source":["## Question 3\n","\n","*   Fit a K-means clustering model with two clusters.\n","*   Examine the clusters by counting the number of red and white wines in each cluster.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T01:59:09.017663Z","start_time":"2017-03-19T21:59:08.896993-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from sklearn.cluster import KMeans\n","### BEGIN SOLUTION\n","km = KMeans(n_clusters=2, random_state=42)\n","km = km.fit(data[float_columns])\n","\n","data['kmeans'] = km.predict(data[float_columns])"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T01:59:09.332043Z","start_time":"2017-03-19T21:59:09.315410-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["(data[['color','kmeans']]\n"," .groupby(['kmeans','color'])\n"," .size()\n"," .to_frame()\n"," .rename(columns={0:'number'}))\n","### END SOLUTION"]},{"cell_type":"markdown","metadata":{},"source":["## Question 4\n","\n","*   Now fit K-Means models with cluster values ranging from 1 to 20.\n","*   For each model, store the number of clusters and the inertia value.\n","*   Plot cluster number vs inertia. Does there appear to be an ideal cluster number?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T01:59:21.588328Z","start_time":"2017-03-19T21:59:12.450288-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["### BEGIN SOLUTION\n","# Create and fit a range of models\n","km_list = list()\n","\n","for clust in range(1,21):\n","    km = KMeans(n_clusters=clust, random_state=42)\n","    km = km.fit(data[float_columns])\n","    \n","    km_list.append(pd.Series({'clusters': clust, \n","                              'inertia': km.inertia_,\n","                              'model': km}))"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T01:59:21.775524Z","start_time":"2017-03-19T21:59:21.589747-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["plot_data = (pd.concat(km_list, axis=1)\n","             .T\n","             [['clusters','inertia']]\n","             .set_index('clusters'))\n","\n","ax = plot_data.plot(marker='o',ls='-')\n","ax.set_xticks(range(0,21,2))\n","ax.set_xlim(0,21)\n","ax.set(xlabel='Cluster', ylabel='Inertia');\n","### END SOLUTION"]},{"cell_type":"markdown","metadata":{},"source":["## Question 5\n","\n","*   Fit an agglomerative clustering model with two clusters.\n","*   Compare the results to those obtained by K-means with regards to wine color by reporting the number of red and white observations in each cluster for both K-means and agglomerative clustering.\n","*   Visualize the dendrogram produced by agglomerative clustering. *Hint:* SciPy has a module called [`cluster.hierarchy`](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01#module-scipy.cluster.hierarchy) that contains the `linkage` and `dendrogram` functions required to create the linkage map and plot the resulting dendrogram.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T11:30:31.254165Z","start_time":"2017-03-20T07:30:27.894864-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from sklearn.cluster import AgglomerativeClustering\n","### BEGIN SOLUTION\n","ag = AgglomerativeClustering(n_clusters=2, linkage='ward', compute_full_tree=True)\n","ag = ag.fit(data[float_columns])\n","data['agglom'] = ag.fit_predict(data[float_columns])"]},{"cell_type":"markdown","metadata":{},"source":["Note that cluster assignment is arbitrary, the respective primary cluster numbers for red and white may not be identical to the ones below and also may not be the same for both K-means and agglomerative clustering.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T11:46:08.938224Z","start_time":"2017-03-20T07:46:08.924114-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["# First, for Agglomerative Clustering:\n","(data[['color','agglom','kmeans']]\n"," .groupby(['color','agglom'])\n"," .size()\n"," .to_frame()\n"," .rename(columns={0:'number'}))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Comparing with KMeans results:\n","(data[['color','agglom','kmeans']]\n"," .groupby(['color','kmeans'])\n"," .size()\n"," .to_frame()\n"," .rename(columns={0:'number'}))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Comparing results:\n","(data[['color','agglom','kmeans']]\n"," .groupby(['color','agglom','kmeans'])\n"," .size()\n"," .to_frame()\n"," .rename(columns={0:'number'}))"]},{"cell_type":"markdown","metadata":{},"source":["Though the cluster numbers are not identical, the clusters are very consistent within a single wine variety (red or white).\n","\n","And here is a plot of the dendrogram created from agglomerative clustering.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2017-03-20T11:53:03.838928Z","start_time":"2017-03-20T07:53:02.088506-04:00"},"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["# First, we import the cluster hierarchy module from SciPy (described above) to obtain the linkage and dendrogram functions.\n","from scipy.cluster import hierarchy\n","\n","Z = hierarchy.linkage(ag.children_, method='ward')\n","\n","fig, ax = plt.subplots(figsize=(15,5))\n","\n","\n","den = hierarchy.dendrogram(Z, orientation='top', \n","                           p=30, truncate_mode='lastp',\n","                           show_leaf_counts=True, ax=ax)\n","### END SOLUTION"]},{"cell_type":"markdown","metadata":{},"source":["## Question 6\n","\n","In this question, we are going to explore clustering as a form of feature engineering.\n","\n","*   Create a **binary** target variable `y`, denoting if the quality is greater than 7 or not.\n","*   Create a variable called `X_with_kmeans` from `data`, by dropping the columns \"quality\", \"color\" and \"agglom\" from the dataset. Create `X_without_kmeans` from that by dropping \"kmeans\".\n","*   For both datasets, using **[StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2022-01-01)** with 10 splits, fit 10 Random Forest Classifiers and find the mean of the ROC-AUC scores from these 10 classifiers.\n","*   Compare the average roc-auc scores for both models, the one using the KMeans cluster as a feature and the one that doesn't use it.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, roc_auc_score\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","### BEGIN SOLUTION\n","y = (data['quality'] > 7).astype(int)\n","X_with_kmeans = data.drop(['agglom', 'color', 'quality'], axis=1)\n","X_without_kmeans = X_with_kmeans.drop('kmeans', axis=1)\n","sss = StratifiedShuffleSplit(n_splits=10, random_state=6532)\n","\n","def get_avg_roc_10splits(estimator, X, y):\n","    roc_auc_list = []\n","    for train_index, test_index in sss.split(X, y):\n","        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","        estimator.fit(X_train, y_train)\n","        y_predicted = estimator.predict(X_test)\n","        y_scored = estimator.predict_proba(X_test)[:, 1]\n","        roc_auc_list.append(roc_auc_score(y_test, y_scored))\n","    return np.mean(roc_auc_list)\n","# return classification_report(y_test, y_predicted)\n","\n","estimator = RandomForestClassifier()\n","roc_with_kmeans = get_avg_roc_10splits(estimator, X_with_kmeans, y)\n","roc_without_kmeans = get_avg_roc_10splits(estimator, X_without_kmeans, y)\n","print(\"Without kmeans cluster as input to Random Forest, roc-auc is \\\"{0}\\\"\".format(roc_without_kmeans))\n","print(\"Using kmeans cluster as input to Random Forest, roc-auc is \\\"{0}\\\"\".format(roc_with_kmeans))"]},{"cell_type":"markdown","metadata":{},"source":["Let's now explore if the number of clusters have an effect in this improvement.\n","\n","*   Create the basis training set from `data` by restricting to float_columns.\n","*   For $n = 1, \\ldots, 20$, fit a KMeans algorithim with $n$ clusters. **[One-hot encode]()** it and add it to the **basis** training set. Don't add it to the previous iteration.\n","*   Fit 10 **Logistic Regression** models and compute the average roc-auc-score.\n","*   Plot the average roc-auc scores.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","X_basis = data[float_columns]\n","sss = StratifiedShuffleSplit(n_splits=10, random_state=6532)\n","\n","def create_kmeans_columns(n):\n","    km = KMeans(n_clusters=n)\n","    km.fit(X_basis)\n","    km_col = pd.Series(km.predict(X_basis))\n","    km_cols = pd.get_dummies(km_col, prefix='kmeans_cluster')\n","    return pd.concat([X_basis, km_cols], axis=1)\n","\n","estimator = LogisticRegression()\n","ns = range(1, 21)\n","roc_auc_list = [get_avg_roc_10splits(estimator, create_kmeans_columns(n), y)\n","                for n in ns]\n","\n","ax = plt.axes()\n","ax.plot(ns, roc_auc_list)\n","ax.set(\n","    xticklabels= ns,\n","    xlabel='Number of clusters as features',\n","    ylabel='Average ROC-AUC over 10 iterations',\n","    title='KMeans + LogisticRegression'\n",")\n","ax.grid(True)\n","### END SOLUTION"]},{"cell_type":"markdown","metadata":{},"source":["***\n","\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}